{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c8e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Obtaining dependency information for pypdf2 from https://files.pythonhosted.org/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17656c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/compat/__init__.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# import libraries\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import pickle\n",
    "import subprocess\n",
    "import nltk\n",
    "nltk.data.path.append('/data1/brandsena/nltk_data/')\n",
    "\n",
    "sys.path.insert(1, '/home/brandsena/timeperiod-to-daterange/')\n",
    "#import timeperiod2daterange\n",
    "\n",
    "# set higher recursion for pypdf2\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "\"\"\"\n",
    "# BERT model import\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "cudaGpuNumber = torch.cuda.current_device()\n",
    "#print('cuda gpu number is '+str(cudaGpuNumber))\n",
    "modelDir = '/data1/brandsena/lucdh-dataset/archeobertje-production-model-fold2/'\n",
    "model = AutoModelForTokenClassification.from_pretrained(modelDir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelDir)\n",
    "predictor = pipeline(\n",
    "                      'ner', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer,\n",
    "                      device = cudaGpuNumber, # 0 for gpu, -1 for cpu\n",
    "                      #device = -1, # 0 for gpu, -1 for cpu\n",
    "                      grouped_entities = False\n",
    "                    )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def download_document(file_url, source, file_id, file_name = False):\n",
    "    # 2DO get pdf location folder from settings file\n",
    "    pdf_folder = \"/data1/brandsena/document-sources/oe_vlaanderen/pdf/\"\n",
    "    \n",
    "    # if file name not specified, use last part of url as filename\n",
    "    if not file_name:\n",
    "        file_name = file_url.split('/')[-1]\n",
    "        \n",
    "    output_location = f\"{pdf_folder}{file_id}_{file_name}\"\n",
    "    \n",
    "    urllib.request.urlretrieve(file_url, output_location) \n",
    "    \n",
    "    return output_location\n",
    "\n",
    "def _mkdir(_dir):\n",
    "    if os.path.isdir(_dir): pass\n",
    "    elif os.path.isfile(_dir):\n",
    "        raise OSError(\"%s exists as a regular file.\" % _dir)\n",
    "    else:\n",
    "        parent, directory = os.path.split(_dir)\n",
    "        if parent and not os.path.isdir(parent): _mkdir(parent)\n",
    "        if directory: os.mkdir(_dir)\n",
    "\n",
    "            \n",
    "def save_json(data, location):\n",
    "\n",
    "    # make parent folder(s) if needed\n",
    "    _mkdir('/'.join(location.split('/')[0:-1]))\n",
    "    \n",
    "    # save json\n",
    "    jsonOutput = json.dumps(data)\n",
    "    with open(location, \"w\") as json_file:\n",
    "        try:\n",
    "            json_file.write(jsonOutput) \n",
    "        except:\n",
    "            jsonOutput = jsonOutput.encode('utf-8')\n",
    "            json_file.write(jsonOutput) \n",
    "\n",
    "def run_ner_on_pdf(fileLocation, outputFolder, language = 'dutch'):\n",
    "    try:\n",
    "        # create a pdf reader object\n",
    "        try:\n",
    "            reader = PdfReader(fileLocation)\n",
    "        except Exception as error:\n",
    "            print('PDF reading error')\n",
    "            print(error)\n",
    "            return False # something wrong with pdf, skip file\n",
    "            \n",
    "        \n",
    "        # loop through pages \n",
    "        for i in range(0, len(reader.pages) ):\n",
    "        \n",
    "            pageNumber = i + 1\n",
    "\n",
    "            #logger.info('working on page '+str(pageNumber)+' of file '+fileLocation)\n",
    "            #print('working on page '+str(pageNumber)+' of file '+fileLocation)\n",
    "            \n",
    "            page = reader.pages[i]\n",
    "            \n",
    "            pageEntities = defaultdict(list)\n",
    "            pageTimespans = []\t\n",
    "            maxYear = False\n",
    "            minYear = False\n",
    "            maxYearExcludingRecent = False\t\n",
    "            \n",
    "            try:\n",
    "                pageText = page.extract_text()\n",
    "            except Exception as error:\n",
    "                print('PDF reading error')\n",
    "                print(error)\n",
    "                return False # something wrong with pdf, skip file\n",
    "            \n",
    "            pageText = pageText.replace('\\n',' ') # replace line endings with spaces\n",
    "                    \n",
    "            # create sentences\n",
    "            #print('make sentences')\n",
    "            sent_text = nltk.sent_tokenize(pageText, language = language) # this gives us a list of sentences\n",
    "            \n",
    "            # loop through sentences\n",
    "            for sentence in sent_text:\n",
    "                #print(sentence)\n",
    "                \n",
    "                # get entities \n",
    "                #print('get entities')\n",
    "                entities = predictor(sentence)\n",
    "                #print('entities retrieved')\n",
    "                \n",
    "                concatenatedEntity = ''\n",
    "                currentLabel = False\n",
    "                prevEntity = {'index':0}\n",
    "                \n",
    "                for entity in entities:\n",
    "                    #print(entity)\n",
    "                    \n",
    "                    if entity['word'] != '[UNK]' and entity['word'] != '[CLS]':\n",
    "                        # beginning of entity\n",
    "                        if entity['word'][:2] != '##' and (entity['entity'][:1] == 'B' or prevEntity['index']+1 < entity['index']):\n",
    "                            # save previous entity\n",
    "                            if currentLabel:\n",
    "                                pageEntities[currentLabel].append(concatenatedEntity)\n",
    "                                if currentLabel == 'PER':\n",
    "                                    timespan = timeperiod2daterange.detection2daterange(concatenatedEntity)\n",
    "                                    if timespan:\n",
    "                                        pageTimespans.append({'startdate':timespan[0],'enddate':timespan[1]})\n",
    "                                        if not minYear or timespan[0] < minYear:\n",
    "                                            minYear = timespan[0]\n",
    "                                        if not maxYear or timespan[1] > maxYear:\n",
    "                                            maxYear = timespan[1]\n",
    "                                        if timespan[1] < 1950 and (not maxYearExcludingRecent or timespan[1] > maxYearExcludingRecent):\n",
    "                                            maxYearExcludingRecent = timespan[1]\n",
    "                                            \n",
    "                            \n",
    "                            # store entity in memory          \n",
    "                            concatenatedEntity = entity['word']\n",
    "                            currentLabel = entity['entity'][2:]\n",
    "                        \n",
    "                        # continuation of word in entity\n",
    "                        elif entity['word'][:2] == '##':\n",
    "                            concatenatedEntity += entity['word'][2:]\n",
    "                            \n",
    "                        # new word in same entity\n",
    "                        else:\n",
    "                            concatenatedEntity += ' '+entity['word']\n",
    "                            \n",
    "                        prevEntity = entity\n",
    "                    \n",
    "            # save as json\n",
    "            #print('save json')\n",
    "            pageJsonOutput = {\n",
    "                \"page_number\":pageNumber,\n",
    "                \"content\":pageText\n",
    "            }\n",
    "            if pageEntities:\n",
    "                pageJsonOutput[\"ner_entities\"] = pageEntities\n",
    "            if pageTimespans:\n",
    "                pageJsonOutput[\"timespans\"] = pageTimespans\n",
    "            if minYear and (maxYear or maxYearExcludingRecent):\n",
    "                pageJsonOutput[\"minYear\"] = minYear\n",
    "                if maxYear:\n",
    "                    pageJsonOutput[\"maxYear\"] = maxYear\n",
    "                if maxYearExcludingRecent:\n",
    "                    pageJsonOutput[\"maxYearExcludingRecent\"] = maxYearExcludingRecent\n",
    "                \n",
    "            \n",
    "            save_json(pageJsonOutput, outputFolder+'/page'+str(pageNumber)+'.json')\n",
    "            \n",
    "            #print (pageJsonOutput)\n",
    "            #print (r.text)\n",
    "    except Exception as error:\n",
    "        print('PDF reading error')\n",
    "        print(error)\n",
    "        \n",
    "def pdf2html(file_location):\n",
    "    \"\"\"convert single file from pdf to html\"\"\"\n",
    "\n",
    "    # do html conversion\n",
    "    # TODO maybe use md5 of file for folder name to stop clashes?\n",
    "    htmlDir = '/data1/brandsena/document-sources/oe_vlaanderen/html/' + file_location.split('/')[-1].replace('.pdf', '')\n",
    "    if not os.path.isdir(htmlDir):\n",
    "        # first create folder for html to go in\n",
    "\n",
    "        _mkdir(htmlDir)\n",
    "\n",
    "        try:\n",
    "            cmnd = 'pdftohtml -c -nodrm ' + file_location.replace(' ', '\\ ') + ' ' + htmlDir + '/index.html'\n",
    "            output = subprocess.check_output(\n",
    "                cmnd, stderr=subprocess.STDOUT, shell=True,\n",
    "                universal_newlines=True)\n",
    "        except subprocess.CalledProcessError as exc:\n",
    "            errormsg = \"pdftohtml error for file \" + file_location + \" \" + exc.output\n",
    "            print (errormsg)\n",
    "            #logger.error(errormsg)\n",
    "            os.rmdir(htmlDir)\n",
    "        else:\n",
    "            print (\"Converted \" + file_location + \" to html!\")\n",
    "\n",
    "    else:\n",
    "        print (file_location + \" html folder already exists, skipping\")\n",
    "        \n",
    "        \n",
    "def rd2wgs (x,y):\n",
    "    \"\"\"Calculate WGS84 coordinates\"\"\"\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "\n",
    "    dX = (x - 155000) * pow(10, - 5)\n",
    "    dY = (y - 463000) * pow(10, - 5)\n",
    "\n",
    "    SomN = (3235.65389 * dY) + (- 32.58297 * pow(dX, 2)) + (- 0.2475 * pow(dY, 2)) + (- 0.84978 * pow(dX, 2) * dY) + (- 0.0655 * pow(dY, 3)) + (- 0.01709 * pow(dX, 2) *pow(dY, 2)) + (- 0.00738 * dX) + (0.0053 * pow(dX, 4)) + (- 0.00039 * pow(dX, 2) *pow(dY, 3)) + (0.00033 * pow(dX, 4) * dY) + (- 0.00012 * dX * dY)\n",
    "\n",
    "    SomE = (5260.52916 * dX) + (105.94684 * dX * dY) + (2.45656 * dX * pow(dY, 2)) + (- 0.81885 * pow(dX, 3)) + (0.05594 * dX * pow(dY, 3)) + (- 0.05607 * pow(dX, 3) * dY) + (0.01199 * dY) + (- 0.00256 * pow(dX, 3) *pow(dY, 2)) + (0.00128 * dX * pow(dY, 4)) + (0.00022 * pow(dY,2)) + (- 0.00022 * pow(dX, 2)) + (0.00026 * pow(dX, 5))\n",
    "\n",
    "    lat = 52.15517 + (SomN / 3600);\n",
    "    lon = 5.387206 + (SomE / 3600);\n",
    "\n",
    "    return lat,lon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = 'pdf/'\n",
    "json_folder = 'json/'\n",
    "archis_zaakdocumenten_location = 'zaakdoc-vondstloc-join.csv'\n",
    "\n",
    "archis_zaakdocumenten = pd.read_csv(archis_zaakdocumenten_location)\n",
    "#print(archis_zaakdocumenten)\n",
    "\n",
    "for directory, subdirectories, files in os.walk(pdf_folder):\n",
    "    for file in files:\n",
    "        \n",
    "        file_location = os.path.join(directory, file)\n",
    "        \n",
    "        print(f\"indexing: {file}\")\n",
    "        \n",
    "        output_folder = json_folder+file.replace('.pdf','')\n",
    "\n",
    "        # if not done yet\n",
    "        if os.path.exists(output_folder):\n",
    "            print('Output folder for '+file+' already exists, skipping')\n",
    "            continue #skip this file if it already exists\n",
    "        else:\n",
    "            # create folder, do try just in case other process has already made the folder\n",
    "            try:\n",
    "                os.mkdir(output_folder)\n",
    "                print('created folder '+output_folder)\n",
    "            except:\n",
    "                print('cannot create folder '+output_folder)\n",
    "                continue\n",
    "        \n",
    "            \n",
    "\n",
    "        archis_zaakidentificatie = int(file.split('_')[0][1:]+'100')\n",
    "        #print(archis_zaakidentificatie)\n",
    "        \n",
    "        record = archis_zaakdocumenten.loc[archis_zaakdocumenten['zaakidentificatie'] == archis_zaakidentificatie]\n",
    "        \n",
    "        if len(record) == 0: # no result in db, log and skip\n",
    "            print(f\"no entry in db for {archis_zaakidentificatie}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        if len(record) > 1: # multiple rows with same zaakidentificatie, get one with document_id\n",
    "            record = record.loc[record['document_id'].notnull()]\n",
    "\n",
    "        \n",
    "        file_name = file.replace(' ','_')\n",
    "\n",
    "        output_document = {}\n",
    "        output_document['source'] = 'archis'\n",
    "        output_document['file_name'] = file_name\n",
    "        output_document['file_type'] = 'report'\n",
    "        output_document['title'] = record['titel'].values[0]\n",
    "        creators = re.split(',|&| en |/|;', record['auteur'].values[0])    # split on muliple characters, because messy data         \n",
    "        output_document['creators'] = creators\n",
    "        output_document['description'] = '' # no descriptions in this data source\n",
    "        output_document['publisher'] = '' # no publisher in this data source\n",
    "        output_document['createdAt'] = int(record['jaar'].values[0])\n",
    "        output_document['identifiers'] = {\n",
    "            'uri': record['link'].values[0],\n",
    "            'archis_zaakidentificatie': int(record['zaakidentificatie'].values[0]),\n",
    "            'archis_zaak_id': int(record['zaak_id'].values[0]),\n",
    "            'archis_identificatie': str(record['identificatie'].values[0])\n",
    "        }\n",
    "        output_document['language'] = 'Dutch'\n",
    "        output_document['html_folder_name'] = f\"{archis_zaakidentificatie}_{file_name.replace('.pdf','')}\"\n",
    "\n",
    "        #coordinates\n",
    "        if str(record['x_coordinaat'].values[0]) != 'nan' and str(record['x_coordinaat'].values[0]) != 'nan':\n",
    "            coordX = int(record['x_coordinaat'].values[0])\n",
    "            coordY = int(record['y_coordinaat'].values[0])\n",
    "            lat, lon = rd2wgs(coordX,coordY)\n",
    "            output_document['coordX'] = coordX\n",
    "            output_document['coordY'] = coordY\n",
    "            output_document['location'] = {'lat':lat,'lon':lon}\n",
    "\n",
    "        # save document.json 2DO get json location from settings\n",
    "        json_output_folder = f\"json/{file_name.replace('.pdf','')}\"\n",
    "        save_json(output_document, f\"{json_output_folder}/document.json\")\n",
    "\n",
    "                   \n",
    "\"\"\"\n",
    "                    # process pdf, store page.json files with entities \n",
    "                    run_ner_on_pdf(\n",
    "                        pdf_location, \n",
    "                        json_output_folder, \n",
    "                        'dutch'\n",
    "                    )\n",
    "\n",
    "                    # make and save html version of pdf\n",
    "                    pdf2html(pdf_location)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archis_zaaknummer = 4000543100\n",
    "print(archis_zaaknummer)\n",
    "record = archis_zaakdocumenten.loc[archis_zaakdocumenten['zaakidentificatie'] == archis_zaaknummer]\n",
    "\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(record) > 1:\n",
    "    record = record.loc[record['document_id'].notnull()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80143aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "record['link'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1250a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
